import asyncio
import time
import uuid
import random
from typing import AsyncGenerator, Dict, Any, List, Optional
from app.contracts.stream_interface import (
    LLMStreamInterface, 
    StreamRequest, 
    StreamChunk, 
    StreamStatus
)

class MockStreamService(LLMStreamInterface):
    """–ú–æ–∫-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è AI —Å—Ç—Ä–∏–º–∞"""
    
    def __init__(self):
        self.supported_models = ["mock-gpt-3.5", "mock-gpt-4", "mock-claude", "mock-llama"]
        self.model_info = {
            "mock-gpt-3.5": {
                "name": "Mock GPT-3.5",
                "description": "Mock implementation of GPT-3.5",
                "max_tokens": 4096,
                "temperature_range": [0.0, 2.0]
            },
            "mock-gpt-4": {
                "name": "Mock GPT-4",
                "description": "Mock implementation of GPT-4",
                "max_tokens": 8192,
                "temperature_range": [0.0, 2.0]
            },
            "mock-claude": {
                "name": "Mock Claude",
                "description": "Mock implementation of Claude",
                "max_tokens": 8192,
                "temperature_range": [0.0, 1.0]
            },
            "mock-llama": {
                "name": "Mock LLaMA",
                "description": "Mock implementation of LLaMA",
                "max_tokens": 2048,
                "temperature_range": [0.0, 1.5]
            }
        }
        
    def _get_mock_responses(self, prompt: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ–∫-–æ—Ç–≤–µ—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–º–ø—Ç–∞"""
        prompt_lower = prompt.lower()
        
        if "python" in prompt_lower or "–∫–æ–¥" in prompt_lower:
            return [
                "–ö–æ–Ω–µ—á–Ω–æ! –í–æ—Ç –ø—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –Ω–∞ Python:\n\n",
                "```python\n",
                "def hello_world():\n",
                "    print('Hello, World!')\n",
                "    return 'Success'\n\n",
                "# –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏\n",
                "result = hello_world()\n",
                "print(f'–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}')\n",
                "```\n\n",
                "–≠—Ç–æ—Ç –∫–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤ Python. ",
                "–§—É–Ω–∫—Ü–∏—è –≤—ã–≤–æ–¥–∏—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."
            ]
        elif "–ø–æ–≥–æ–¥–∞" in prompt_lower or "weather" in prompt_lower:
            return [
                "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —É –º–µ–Ω—è –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º –æ –ø–æ–≥–æ–¥–µ. ",
                "–û–¥–Ω–∞–∫–æ —è –º–æ–≥—É –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ —É–∑–Ω–∞—Ç—å –ø–æ–≥–æ–¥—É:\n\n",
                "1. –í–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–∞–π—Ç–∞–º–∏: weather.com, gismeteo.ru\n",
                "2. –ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø–æ–≥–æ–¥—ã\n",
                "3. –ì–æ–ª–æ—Å–æ–≤—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ (Siri, Google Assistant)\n",
                "4. API —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ"
            ]
        elif "–ø—Ä–∏–≤–µ—Ç" in prompt_lower or "hello" in prompt_lower:
            return [
                "–ü—Ä–∏–≤–µ—Ç! üëã –†–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å! ",
                "–ö–∞–∫ –¥–µ–ª–∞? –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? ",
                "–ì–æ—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Ç–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø–æ–º–æ—á—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏. ",
                "–ü—Ä–æ—Å—Ç–æ —Å–ø—Ä–æ—Å–∏ –æ —á—ë–º —É–≥–æ–¥–Ω–æ!"
            ]
        elif "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞" in prompt_lower or "math" in prompt_lower:
            return [
                "–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ - —ç—Ç–æ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞—É–∫–∞! ",
                "–í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤:\n\n",
                "‚Ä¢ –ß–∏—Å–ª–æ œÄ (–ø–∏) —Å–æ–¥–µ—Ä–∂–∏—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ñ—Ä\n",
                "‚Ä¢ –ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ œÜ ‚âà 1.618 –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –ø—Ä–∏—Ä–æ–¥–µ\n",
                "‚Ä¢ –¢–µ–æ—Ä–µ–º–∞ –ü–∏—Ñ–∞–≥–æ—Ä–∞: a¬≤ + b¬≤ = c¬≤\n",
                "‚Ä¢ –§–æ—Ä–º—É–ª–∞ –≠–π–ª–µ—Ä–∞: e^(iœÄ) + 1 = 0\n\n",
                "–° –∫–∞–∫–æ–π –æ–±–ª–∞—Å—Ç—å—é –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ —Ç–µ–±–µ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å?"
            ]
        else:
            return [
                "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å! ",
                "–ü–æ–∑–≤–æ–ª—å—Ç–µ –º–Ω–µ –ø–æ–¥—É–º–∞—Ç—å –Ω–∞–¥ —ç—Ç–∏–º... ",
                "–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤–∞—à–µ–º –∑–∞–ø—Ä–æ—Å–µ, –º–æ–≥—É —Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ:\n\n",
                "–≠—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ —Å–ª–æ–∂–Ω–∞—è —Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è. ",
                "–†–µ–∫–æ–º–µ–Ω–¥—É—é –∏–∑—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ",
                "–∏ –ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º–∏ –≤ –¥–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. ",
                "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, ",
                "—è –±—É–¥—É —Ä–∞–¥ –ø–æ–º–æ—á—å —Å –Ω–∏–º–∏!"
            ]

    async def stream_generate(
        self, 
        request: StreamRequest
    ) -> AsyncGenerator[StreamChunk, None]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–∏–º–∞ —Å –º–æ–∫-–¥–∞–Ω–Ω—ã–º–∏"""
        
        stream_id = str(uuid.uuid4())
        start_time = time.time()
        
        # –°—Ç–∞—Ä—Ç–æ–≤—ã–π —á–∞–Ω–∫
        yield StreamChunk(
            content="",
            status=StreamStatus.STARTED,
            chunk_id=f"{stream_id}_start",
            timestamp=start_time,
            metadata={
                "stream_id": stream_id,
                "model": request.model or "mock-gpt-3.5",
                "temperature": request.temperature
            }
        )
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        await asyncio.sleep(0.5)
        
        try:
            mock_responses = self._get_mock_responses(request.prompt)
            
            for i, response_part in enumerate(mock_responses):
                # –†–∞–∑–±–∏–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –Ω–∞ —Å–ª–æ–≤–∞ –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∞
                words = response_part.split()
                
                for j, word in enumerate(words):
                    chunk_content = word + " " if j < len(words) - 1 else word
                    
                    yield StreamChunk(
                        content=chunk_content,
                        status=StreamStatus.STREAMING,
                        chunk_id=f"{stream_id}_{i}_{j}",
                        timestamp=time.time(),
                        metadata={
                            "part_index": i,
                            "word_index": j,
                            "total_parts": len(mock_responses)
                        }
                    )
                    
                    # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
                    delay = random.uniform(0.05, 0.3)
                    await asyncio.sleep(delay)
            
            # –ó–∞–≤–µ—Ä—à–∞—é—â–∏–π —á–∞–Ω–∫
            completion_time = time.time() - start_time
            yield StreamChunk(
                content="",
                status=StreamStatus.COMPLETED,
                chunk_id=f"{stream_id}_end",
                timestamp=time.time(),
                metadata={
                    "completion_time": completion_time,
                    "total_tokens": sum(len(part.split()) for part in mock_responses),
                    "model_used": request.model or "mock-gpt-3.5"
                }
            )
            
        except Exception as e:
            yield StreamChunk(
                content="",
                status=StreamStatus.ERROR,
                chunk_id=f"{stream_id}_error",
                timestamp=time.time(),
                metadata={
                    "error": str(e),
                    "error_type": type(e).__name__
                }
            )

    async def generate(self, request: StreamRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞"""
        mock_responses = self._get_mock_responses(request.prompt)
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_time = random.uniform(0.5, 2.0)
        await asyncio.sleep(processing_time)
        
        return "".join(mock_responses)

    def get_supported_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return self.supported_models.copy()

    async def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–∫-—Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "status": "healthy",
            "service": "MockStreamService",
            "version": "1.0.0",
            "models_available": len(self.supported_models),
            "uptime": "99.9%",
            "response_time_ms": random.randint(50, 200),
            "timestamp": time.time()
        }

    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return self.model_info.get(model_name)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    
    async def get_conversation_summary(self, messages: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –±–µ—Å–µ–¥—ã"""
        await asyncio.sleep(0.5)  # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        total_messages = len(messages)
        total_words = sum(len(msg.split()) for msg in messages)
        
        return f"–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –±–µ—Å–µ–¥—ã: {total_messages} —Å–æ–æ–±—â–µ–Ω–∏–π, ~{total_words} —Å–ª–æ–≤. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã: –æ–±—â–µ–Ω–∏–µ, –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã."

    async def suggest_next_questions(self, context: str) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        await asyncio.sleep(0.3)
        
        context_lower = context.lower()
        
        if "python" in context_lower:
            return [
                "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –≤ Python?",
                "–ß—Ç–æ —Ç–∞–∫–æ–µ list comprehensions?",
                "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å async/await?",
                "–û–±—ä—è—Å–Ω–∏ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ –∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏"
            ]
        elif "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞" in context_lower:
            return [
                "–ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—ã?",
                "–ö–∞–∫ —Ä–µ—à–∞—Ç—å —Å–∏—Å—Ç–µ–º—ã —É—Ä–∞–≤–Ω–µ–Ω–∏–π?",
                "–û–±—ä—è—Å–Ω–∏ —Ç–µ–æ—Ä–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π",
                "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—Ç—Ä–∏—Ü—ã –∏ –∫–∞–∫ —Å –Ω–∏–º–∏ —Ä–∞–±–æ—Ç–∞—Ç—å?"
            ]
        else:
            return [
                "–ú–æ–∂–µ—à—å —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ?",
                "–ü—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "–ö–∞–∫–∏–µ –µ—Å—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã?",
                "–ì–¥–µ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ?"
            ]

    def get_usage_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–º–æ–∫-–¥–∞–Ω–Ω—ã–µ)"""
        return {
            "total_requests": random.randint(1000, 5000),
            "successful_streams": random.randint(900, 4500),
            "failed_requests": random.randint(10, 100),
            "average_response_time": random.uniform(0.5, 2.0),
            "most_used_model": random.choice(self.supported_models),
            "uptime_percentage": random.uniform(95.0, 99.9)
        }